<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: light)">
<meta name="theme-color" content="#222" media="(prefers-color-scheme: dark)">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&family=Noto+Sans+Mono+CJK+SC:ital,wght@0,300;0,400;0,700;1,300;1,400;1,700&display=swap&subset=latin,latin-ext">

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/7.0.0/css/all.min.css" integrity="sha256-VHqXKFhhMxcpubYf9xiWdCiojEbY9NexQ4jh8AxbvcM=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"ariasuko.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":true,"version":"8.26.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"post","padding":18,"offset":12},"hljswrap":true,"codeblock":{"theme":{"light":"default","dark":"stackoverflow-dark"},"prism":{"light":"prism","dark":"prism-dark"},"copy_button":{"enable":true,"style":"default"},"fold":{"enable":true,"height":500},"language":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"duration":200,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"}}</script><script src="/js/config.js" defer></script>

    <meta property="og:type" content="website">
<meta property="og:title" content="Suko Aria&#39;s blog">
<meta property="og:url" content="http://ariasuko.github.io/index.html">
<meta property="og:site_name" content="Suko Aria&#39;s blog">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="Suko Aria">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="http://ariasuko.github.io/">


<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":true,"isPost":false,"lang":"zh-CN","comments":"","permalink":"","path":"index.html","title":""}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>Suko Aria's blog</title>
  








  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous" defer></script>
<script src="/js/utils.js" defer></script><script src="/js/motion.js" defer></script><script src="/js/sidebar.js" defer></script><script src="/js/next-boot.js" defer></script>

  






  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"all","js":{"url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.js","integrity":"sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI="}}</script>
<script src="/js/third-party/math/mathjax.js" defer></script>



  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">Suko Aria's blog</h1>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">papers overflowing......</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-overview-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Suko Aria"
      src="/uploads/avatar.jpg">
  <p class="site-author-name" itemprop="name">Suko Aria</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Ariasuko" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Ariasuko" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>
  <div class="cc-license animated" itemprop="license">
    <a href="https://creativecommons.org/licenses/by-nc/4.0/" class="cc-opacity" rel="noopener" target="_blank"><img src="https://cdnjs.cloudflare.com/ajax/libs/creativecommons-vocabulary/2020.11.3/assets/license_badges/small/by_nc.svg" alt="Creative Commons"></a>
  </div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner index posts-expand">

    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://ariasuko.github.io/2026/how-to-install-megatron-lm/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Suko Aria">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Suko Aria's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Suko Aria's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2026/how-to-install-megatron-lm/" class="post-title-link" itemprop="url">这份文档文档能让你成为Megatron-LM安装与卸载大师</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2026-01-19 19:50:49" itemprop="dateCreated datePublished" datetime="2026-01-19T19:50:49+08:00">2026-01-19</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2026-01-20 13:38:00" itemprop="dateModified" datetime="2026-01-20T13:38:00+08:00">2026-01-20</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文档着重于<code>Megatron-LM</code>的安装指导与一些排错的技巧，并对该软件给出一个当前安装成功率比较高的组合。</p>
<p>Python不时会出现新的版本管理软件，安装上述软件的方式多数基本也大同小异；当然，随着megatron-core的依赖项的变迁，会调起本地编译的包也在增多，实在没必要把所有情况都面面俱到；但只要把步骤方法要点及原理讲明白，其实这些都是触类旁通的。掌握了文档中这些技巧与方法，以后面对新依赖报错也是可以快速上手的，如有疑难欢迎提交相关issues给AI。</p>
<p>此文档针对的受众是追求快速搞好Megatron-LM环境的相关人群等，不过也因本人知识水平有限，同时也避免重复的“造轮子”，对具体报错的理解和处理等方面需要参考AI回答或阅读网上的一些相关文章，因此允许演绎该作及共享也想不到谁要商用。</p>
<p>由于需要复现一些论文中的相关代码与结果等等；仅此，配置Megatron-LM只是为研究需要，以下是免责声明：</p>
<ul>
<li>本文档不保证内容的<strong>实效性</strong>，有问题欢迎找AI深入探讨</li>
<li>这个前言其实一点有用的东西都没有</li>
</ul>
<p>此文档献给被Megatron-LM环境的配置折磨的死去活来的人们，以上…</p>
<h1 id="核心问题">核心问题</h1>
<p>安装Megatron-LM会面临的核心问题，其实是安装过程中会卡在</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Building wheels for collected packages: causal-conv1d, mamba-ssm, nv-grouped-gemm, transformer_engine_torch</span><br></pre></td></tr></table></figure>
<p>这一步骤上。而这个问题究其本质，其实就是连不上在海外的github的分发服务器（CDN）。一而言之，不使用<a
target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/PyTorch">官方的Docker容器</a>，直接在服务器上配置环境，面临的核心问题实际上<strong>网络</strong>的问题。</p>
<p>为了解决这个问题，一般而言就是挂个代理，但是这在服务器上是不现实的。对此有一个替代方案，就是手动去下载wheel预编译文件，然后本地安装，进而把问题转化为找到上述卡住的依赖库提前编译好的wheel文件，它们依赖的目标版本都恰好都满足的版本组合。同时，因为这些预编译文件都放在github上，可以使用一些代理网站在服务器上直接下载，而不是本地上传给服务器，这样可以有效地利用服务器的下载带宽。</p>
<p>TL;DR的话，在文档撰写的时间点，笔者大致检查过的几个最新的版本的可行组合是：（包括但不限于）</p>
<ul>
<li>python=3.11,3.12</li>
<li>torch==2.6.0,2.7.0</li>
</ul>
<p>本文会介绍<del>实际安装过的</del>python3.11+torch2.6.0的过程和中途可能遇到的问题，接下来是一些常见的手动代理手段介绍。</p>
<h1 id="手动代理">手动代理</h1>
<p>为了本文档的完整性，这里附上一份手动代理的方法介绍。所有代理网站和镜像站都以<code>www.abc.xyz</code>代替，具体的代理网站就有请读者自行搜索了，后文给出的下载命令也不会直接添加代理。</p>
<h2 id="python">Python</h2>
<p>虽然称不上手动代理，但是确实有一些不太走心的云服务提供商不修改为国内镜像源的情况，这里一并介绍了。</p>
<p>对于<strong>单次下载</strong>，pip提供了选项<code>-i</code>
<code>--index-url</code>，用法如下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install &lt;package_name&gt; -i https://www.abc.xyz/simple</span><br></pre></td></tr></table></figure>
<p>如果想一次配置，多次使用，python也提供了相关命令：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip config set global.index-url https://www.abc.xyz/simple</span><br></pre></td></tr></table></figure>
<p>该命令的效果等同于修改<code>~/.pip/pip.conf</code>：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[<span class="string">global</span>]</span><br><span class="line"><span class="string">index-url</span> <span class="string">=</span> <span class="string">https://www.abc.xyz/simple</span></span><br></pre></td></tr></table></figure>
<p>另外提一句如果使用了<a
target="_blank" rel="noopener" href="https://docs.astral.sh/uv/">uv</a>作为版本控制软件，它的pip兼容命令在单次下载时可以继续使用<code>-i</code>选项；但是由于不存在config子命令，并且目前uv会忽略pip.config配置文件，一次配置多次使用的方法略有改变，而且没有配置全局的方法，只能在项目文件中添加如下项目：</p>
<figure class="highlight toml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="section">[[tool.uv.index]]</span></span><br><span class="line"><span class="attr">url</span> = <span class="string">&quot;htts://www.abc.xyz/simple&quot;</span></span><br><span class="line"><span class="attr">default</span> = <span class="literal">true</span></span><br></pre></td></tr></table></figure>
<h2 id="github">Github</h2>
<p>对于Github，本文主要涉及的需求有两个：下载仓库和下载该项目的release。主流的解决方案是用互联网大善人&amp;慈善家Cloudflare家的workers，当然我们直接用别人现成配置好的即可。至于使用方法，只需要在原始链接前面加上你想使用代理网站：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">下载仓库</span></span><br><span class="line">git clone https://www.abc.xyz/https://github.com/xxx/xxx.git</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">下载release</span></span><br><span class="line">wget &quot;https://www.abc.xyz/https://github.com/xxx/xxx/release/x.x.x/xxx.whl&quot;</span><br></pre></td></tr></table></figure>
<p>当然，还有一种方案就是，去找这个仓库有没有有人搬到Gitee上了，不过一般只能下载这个仓库，release的话Gitee是不会镜像过来的。</p>
<h2 id="hugginface">Hugginface</h2>
<p>对于Hugginface则分为两种情况，一是通过Python调用transformer库来下载模型和其它文件（使用他们家的<code>hugginface-cli</code>也属于这种情况），另一种情况是<del>听信AI谗言</del>直接通过wget等工具直接下载相关文件。</p>
<p>对于前者，官方提供一个非常简单环境变量HF_ENDPOINT来使用镜像站（所以实际链接也不是经典的代理格式），此时具体操作为为：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export HF_ENDPOINT=https://www.abc.xyz/</span><br></pre></td></tr></table></figure>
<p>如果是想通过直链下载文件，则需要把域名替换掉：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">https://huggingface.co/xxx/xxx -&gt; https://www.abc.xyz/xxx/xxx</span><br></pre></td></tr></table></figure>
<h1 id="在安装megatron-lm之前">在安装Megatron-LM之前</h1>
<h2 id="前置工作">前置工作</h2>
<p>首先确认自己的python版本是否是需要的版本，若不是则要切换版本。以使用conda安装python
3.12为例：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install python=3.12</span><br></pre></td></tr></table></figure>
<p>然后还有一部分是为了安装megatron-core所需要的依赖：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install pybind11 &quot;setuptools&lt;80.0.0,&gt;=77.0.0&quot; &quot;packaging&gt;=24.2&quot;</span><br></pre></td></tr></table></figure>
<h2 id="安装pytorch">安装PyTorch</h2>
<p>有一点是在安装megatron-core时，它的安装脚本需要确定环境中PyTorch的版本（即Megatron-LM安装选项<code>--no-build-isolation</code>的作用），因此需要提前安装好PyTorch。首先使用命令<code>nvidia-smi</code>检查服务器的显卡驱动，CUDA支持的版本是否大于等于PyTorch需要的版本，对于举例的2.6.0版本，可以通过一下命令指定版本安装：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install torch==2.6.0</span><br></pre></td></tr></table></figure>
<p>上述命令安装的PyTorch的CUDA版本为应该为12.4。如果发现服务器的驱动CUDA小于该版本则比较麻烦，但推荐最好还是更新一下驱动，根据Linux发行版不同方法有所不同，本文档就略过了。</p>
<details>
<summary>
如果读者选择了torch版本不为2.6.0，或者想了解进一步了解CUDA版本的兼容逻辑
</summary>
<p>这里涉及四个CUDA版本：</p>
<ol type="1">
<li>PyTorch支持的版本</li>
<li>系统中CUDA工具链的版本</li>
<li>随着PyTorch下载下来的CUDA pip wheel版本</li>
<li>GPU驱动支持的版本</li>
</ol>
<p><strong>PyTorch支持的版本</strong>，本质上是指的是这个wheel包在预编译时链接的CUDA工具链的版本，可以通过调用<code>torch.version.cuda</code>获取该信息：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python -c &quot;import torch; print(torch.version.cuda)&quot; #torch==2.6.0时默认为12.4</span><br></pre></td></tr></table></figure>
<p><strong>CUDA工具链</strong>包括编译器<code>nvcc</code>和对应的运行库（<code>.so</code>动态链接库+<code>.h</code>头文件）。PyTorch在安装时同时携带一份和它编译版本一致的动态链接库，这些文件是直接打包进PyTorch的wheel文件之中的。而在PyTorch
2.0之后，为了支持图编译和自动算子融合，进而引入了OpenAI
Triton后端，triton则需要CUDA pip wheel。</p>
<p><strong>CUDA Pip
Wheel</strong>则是NVIDIA官方提供的一个方案，它解决就是Python程序需要在运行时现写好一份编译好的GPU机器指令时，但是系统不一定有CUDA运行库的需求。它实际上就是通过pip
wheel包来分发CUDA运行库。通过<code>pip list | grep nvidia</code>可以查看它们的版本号<del>虽然没看出来有什么意义</del>。</p>
<p><strong>GPU驱动支持的版本</strong>最后决定了具体调用的CUDA运行库是否兼容。总所周知CUDA的地基是C++语言，而众所周知C++动态链接库的兼容性是一个非常严肃的玄学问题，最终取决于CUDA工具链和GPU驱动跨版本的兼容性。对此NVIDIA<a
target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/conclusion.html">官方的结论</a>是CUDA驱动程序保持向后兼容性。</p>
<p>这也就说，<ins><strong>需要满足 PyTorch支持的版本 &lt;=
(CUDA工具链版本，后文的Apex需要) &lt;= GPU驱动支持的版本
</strong></ins></p>
<hr />
</details>
<h2 id="安装-ninja-build">安装 ninja-build</h2>
<p>还有一个比较重要的东西是<a
target="_blank" rel="noopener" href="https://ninja-build.org/">ninja-build</a>编译后端（作用相当于<code>make</code>）。它的意义在于让涉及C/C++的编译过程尽可能的快，最显著的一点就是它能自动执行多线程编译。安装ninja不仅可以在可以在安装Megatron-LM和Apex极大的加快编译过程，而且在PyTorch的运行过程中也可以加速一些编译过程（类似JIT）。</p>
<p>包名根据Linux发行版不同也有所不同，但可以用系统的包管理器试一试安装<code>ninja-build</code>或者<code>ninja</code>：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">例如对于Debian/Ubuntu发行版</span></span><br><span class="line">apt install ninja-build</span><br></pre></td></tr></table></figure>
<p>具体的安装说明可以参见<a
target="_blank" rel="noopener" href="https://github.com/ninja-build/ninja/wiki/Pre-built-Ninja-packages">官方在Github上的安装文档</a>。</p>
<h2 id="其它">其它</h2>
<p>其余是一些可以提前安装好的依赖：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tensorboard #如果需要使用集成的tensorboard功能则需要手动下载</span><br></pre></td></tr></table></figure>
<p>然后就可以正式进入Megatron-LM的安装了。</p>
<h1 id="安装-megatron-lm">安装 Megatron-LM</h1>
<p>需要解释的是Megatron-LM实际上指的是megatron-core的额外依赖选项mlm，同时NVIDIA提供了dev和lts两种依赖策略。其中lts+NGC可以看作是NVIDIA为了追求结果可复现性的努力。总结的话，有5种可行组合：</p>
<ul>
<li>（不带额外依赖项）：只有megatron-core和torch</li>
<li>[dev]：追随最新上游依赖项</li>
<li>[lts]：NGC（官方Docker容器）PyTorch 24.01的长期支持版本</li>
<li>[mlm,dev]：（在dev的基础上），能够运行Megatron-LM程序
<del>笔者也不知道这个指的是什么</del></li>
<li>[mlm,lts]：同上</li>
</ul>
<p>本着利在千秋的思想，一般会选择的是更新、支持的更多的版本：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install --no-build-isolation megatron-core[mlm,dev]</span><br></pre></td></tr></table></figure>
<p>当然此时直接执行安装命令，实际上还是会卡在文章开头的所述的环节上的。对此，我们可以去手动下载恰好满足<code>python=3.11</code>&amp;<code>torch=2.6.0+cu12</code>的wheel预编译包：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">causal-conv1d</span></span><br><span class="line">wget https://github.com/Dao-AILab/causal-conv1d/releases/download/v1.5.3.post1/causal_conv1d-1.5.3.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">mamba-ssm</span></span><br><span class="line">wget https://github.com/state-spaces/mamba/releases/download/v2.3.0/mamba_ssm-2.3.0+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">nv_grouped_gemm</span></span><br><span class="line">wget https://github.com/fanshiqing/grouped_gemm/releases/download/v1.1.4.post8/nv_grouped_gemm-1.1.4.post8+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl</span><br></pre></td></tr></table></figure>
<p>对于<code>transformer_engine_torch</code>则没有对应的预编译包，但该包的编译一般也不会卡住。在下载好之后就可以直接<code>pip install 本地文件</code>完成安装。之后再执行megatron-core的安装大概率就不会出现问题了。</p>
<p>以及，如果没有选择举例Python和Torch版本的情况下，预编译包的直链显然也是会不同的。同时为了文档面向未来的兼容性，这里大致讲解一下两个方法。第一种方法就是，直接尝试安装megatron-core，然后等待安装命令超时，之后可以在报错信息中看到具体的下载链接。第二种则是自行前往github发布页，根据本地的实际情况选择依赖包，如果看见包名中的<code>abi</code>默认选<code>FALSE</code>的即可。</p>
<p>不过如果本地选择的版本确实太新的话，就会导致没有对应的预编译包下载。此时pip安装程序会尝试拉取git仓库进行本地编译，然而这个动作多半也会卡住。解决方法也是自行通过代理<code>git clone</code>然后本地编译了。这种方式过于复杂，也确实没有带来明显的好处，如果读者确实需要则只能请教AI了。</p>
<h1 id="安装-apex">安装 Apex</h1>
<p>在安装Apex之前首先需要保证本地有CUDA和NCCL编译工具链。需要额外提醒的是NCCL编译工具链是需要单独安装的，若没有会报错找不到<code>nccl.h</code>。具体安装操作根据选取的Linux发行版有所不同，但网上可以搜索到大量的教程，本文档就也略过了。</p>
<p>首先需要说明的一点，直接<code>pip install apex</code>下载下来的是<a
target="_blank" rel="noopener" href="https://pypi.org/project/apex/">另一个包</a>。实际所需的apex包只有拉取git仓库并编译的一个方法，不过安装过程是简单的，前往它的<a
target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex">Github主页</a>根据指引可以轻松完成安装。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">git clone https://github.com/NVIDIA/apex</span><br><span class="line">cd apex</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">多线程编译，并且安装所有扩展。</span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">APEX_PARALLEL_BUILD=8</span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">NVCC_APPEND_FLAGS=<span class="string">&quot;--threads 4&quot;</span></span> </span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">具体扩展的选择方法见Github上的readme.md</span></span><br><span class="line">NVCC_APPEND_FLAGS=&quot;--threads 4&quot; \</span><br><span class="line">    APEX_PARALLEL_BUILD=8 \</span><br><span class="line">    APEX_CPP_EXT=1 APEX_CUDA_EXT=1 APEX_ALL_CONTRIB_EXT=1 \</span><br><span class="line">    pip install -v --no-build-isolation .</span><br></pre></td></tr></table></figure>
<p>这里可能会出一个问题，那就是在PyTorch的CUDA版本不等于CUDA工具链的版本时，安装apex的脚本应该会发出一个警告并退出。如果CUDA工具链的版本更高+低于驱动支持的CUDA版本的话，临时的解决方案是打开本地Apex的安装脚本<code>setup.py</code>，找到<code>check_cuda_...()</code>函数的调用并注释该行。同样的，不保证实际运行时不会出错。</p>
<h1 id="安装-flash_attn">安装 flash_attn</h1>
<p>这是针对训练脚本中，启用了<strong>Flash
Attention</strong>的情况(<code>--attention-backend</code>)的额外章节，之所以会有该章节是为了提醒<ins><strong>目前</strong></ins>在<code>python=3.11</code>&amp;<code>torch=2.6.0+cu12</code>的情况下：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install flash_attn</span><br></pre></td></tr></table></figure>
<p>自动选取的<code>flash_attn==2.8.3</code>预编译包有bug，具体表现为<a
target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/issues/2010">符号链接出错</a>，可以考虑手动选取<code>2.7.4.post1</code>等较旧的版本。</p>
<h1 id="如何卸载">如何卸载</h1>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm -rf .venv</span><br></pre></td></tr></table></figure>
<h1 id="参考文档">参考文档</h1>
<ul>
<li>Megatron-LM的<a
target="_blank" rel="noopener" href="https://github.com/NVIDIA/Megatron-LM">Github主页</a></li>
<li>Apex的<a target="_blank" rel="noopener" href="https://github.com/NVIDIA/apex">Github主页</a></li>
<li>NVIDIA官方的Megatron-LM<a
target="_blank" rel="noopener" href="https://docs.nvidia.com/megatron-core/developer-guide/latest/user-guide/index.html">用户手册</a></li>
<li>ninja-build的<a target="_blank" rel="noopener" href="https://ninja-build.org/">官网</a></li>
<li>清华镜像源针对配置pip的<a
target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/help/pypi/">帮助</a></li>
<li>uv有关设置index的<a
target="_blank" rel="noopener" href="https://docs.astral.sh/uv/concepts/indexes/">文档</a></li>
<li>PyTorch的<a
target="_blank" rel="noopener" href="https://pytorch.org/get-started/previous-versions/">历史版本安装文档</a>和<a
target="_blank" rel="noopener" href="https://docs.pytorch.org/docs/stable/index.html">API文档</a></li>
<li>PyTorch引入<code>torch.complie</code>的<a
target="_blank" rel="noopener" href="https://pytorch.org/blog/pytorch-2.0-release/">Blog</a></li>
<li>CUDA Pip Wheels的<a
target="_blank" rel="noopener" href="https://docs.nvidia.com/cuda/cuda-installation-guide-linux/index.html#pip-wheels">文档</a></li>
<li>CUDA的<a
target="_blank" rel="noopener" href="https://docs.nvidia.com/deploy/cuda-compatibility/index.html">兼容性文档</a></li>
<li>flash_attn在Github上关于<a
target="_blank" rel="noopener" href="https://github.com/Dao-AILab/flash-attention/issues/2010"><code>symbol not defined linker errors</code>的issue</a></li>
<li>早知道，还得是<a
target="_blank" rel="noopener" href="https://catalog.ngc.nvidia.com/orgs/nvidia/containers/PyTorch">官方的Docker容器</a></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>




    


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="">
    <link itemprop="mainEntityOfPage" href="http://ariasuko.github.io/2025/amp_study/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/uploads/avatar.jpg">
      <meta itemprop="name" content="Suko Aria">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Suko Aria's blog">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content=" | Suko Aria's blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          <a href="/2025/amp_study/" class="post-title-link" itemprop="url">AMP论文阅读</a>
        </h2>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2025-11-26 18:27:15" itemprop="dateCreated datePublished" datetime="2025-11-26T18:27:15+08:00">2025-11-26</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2025-12-15 15:27:12" itemprop="dateModified" datetime="2025-12-15T15:27:12+08:00">2025-12-15</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
          <p>本文是论文 <strong>AMP: Automatically Finding Model Parallel
Strategies with Heterogeneity Awareness (NeurIPS 2022)</strong>
的研读笔记<del>读后感</del></p>
<p><a target="_blank" rel="noopener" href="https://arxiv.org/pdf/2210.07297">论文地址</a> | <a
target="_blank" rel="noopener" href="https://github.com/DachengLi1/AMP">源码仓库</a></p>
<h1 id="tldr">TL;DR</h1>
<p>启发式算法确定3D并行策略，本文设计的动态规划算法确定流水线并行最优划分，使用提前采样的代价预测训练时间。</p>
<h1 id="摘要">摘要</h1>
<p>（方便书写，默认实际的计算设备是<strong>显卡（GPU）</strong>）</p>
<p>论文在分布式训练，在同构、节点异构和模型异构三种环境下，提出了一个确定以下6个超参数的策略：</p>
<ol type="1">
<li><strong>数据并行的维度</strong></li>
<li><strong>张量并行的维度</strong></li>
<li><strong>流水线并行的维度</strong></li>
<li><strong>微批次（micro batchsize）大小</strong></li>
<li><strong>设备布局</strong>（并行策略中哪些显卡视为一组，之间流水线并行，之内张量并行+流水线并行）</li>
<li><strong>流水线层级分配</strong>
（流水线并行中，哪几层（layer）组合为一个阶段（stage），及以这个阶段由哪一张显卡负责）</li>
</ol>
<p>并同时预测实际的训练时间。</p>
<h1 id="方法">方法</h1>
<p>作为概览的补充，先解释论文试图解决了一个什么样的问题，在此之上提出了一个什么样的模型。</p>
<h2 id="提出问题">提出问题</h2>
<p>首先，要训练哪一个模型、训练模型的集群的配置信息和<strong>全局批次大小</strong>（global
batchsize）这三个信息是已知的。这里需要指出的是在论文的方法下，全局批次大小还是需要由模型训练者的经验给出。其次是集群虽然一般选择的空间不多，但是需要提前获取：</p>
<ol type="1">
<li>集群各个节点内显卡间的通信带宽，节点间的通信带宽（或者说网速）。</li>
<li>论文进一步需要各节点的计算能力（这里提前留一个伏笔）。</li>
</ol>
<p>如果把显卡想象成点，点的权就是显卡的计算能力；显卡间的通信带宽想象成线，线的权就是两张卡的实际通信带宽（当然这里有同节点和不同节点两种情况）。如果不考虑如何确定微批次大小（影响流水线并行中的气泡）、模型划分之后实际上能不能装进显卡的显存里等一些分布式训练独有的问题，那论文试图解决的问题大致上就可以看作是一个拓扑图中如何连接各点，优化关键路径的问题。</p>
<p>更细节的解释和准确的符号定义可以参考论文中3.1节。</p>
<h2 id="论文的策略">论文的策略</h2>
<p>这里就来到论文的重点部分了。</p>
<p>论文指出了分布式训练的两个特点：</p>
<ul>
<li>一是通过实际跑一轮（epoch）来获取选取的策略的时间开销不够现实，现有的模型运行一轮的时间实在是太长了；</li>
<li>二是作为一个离散的规划问题，它的解空间非常大的同时，离散的性质阻碍了使用以梯度为基底的优化方法。</li>
</ul>
<p>针对上述特点，论文分别提出了对应的解决方法。对于特点一，论文提出了一个能快速估算训练一轮时间的代价模型。而对于特点二，论文提出了一个动态规划算法，用以提前剔除那些没希望的解（也就是搜素算法+剪枝<del>但感觉有点强行蹭概念</del>）。</p>
<p>下列伪代码是论文设计的策略步骤： <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// 集群信息, 神经网络信息, 全局批次大小, budget</span><br><span class="line">Input: C, W, gbs, budget</span><br><span class="line">degrees = enumerate_degrees(C) //穷举所有可能3D并行策略</span><br><span class="line">record = <span class="built_in">set</span>() // 用于保存结果</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> degrees do</span><br><span class="line">  /* 遍历给定数据并行维度下的可行微批次大小 */</span><br><span class="line">  <span class="keyword">for</span> mbs <span class="keyword">in</span> enumerate_(d.dp) do</span><br><span class="line">    p = placement(C, d) // 启发式设备划分方法</span><br><span class="line">    a = pipe_ast(W, d.pp, p) // 论文提出的最佳流水线划分动态规划算法</span><br><span class="line">    s = (a, mbs, d, p)</span><br><span class="line">    cost = estimate(a, mbs, d, p) // 论文提出的代价模型</span><br><span class="line">    record.add(s, cost)</span><br><span class="line">/* 实际运行前budget个预测结果，以获取实际训练时间 */</span><br><span class="line">best_s = run(record, budget)</span><br><span class="line"><span class="keyword">return</span> best_s</span><br></pre></td></tr></table></figure></p>
<p>其中的启发式算法是引用了一篇参考文献，这里就<del>懒得</del>不做展开了。接下来按顺序分别介绍论文设计的代价模型，和动态规划算法实际解决了什么问题。</p>
<h2 id="代价模型自顶向下的分解">代价模型：自顶向下的分解</h2>
<p>论文对于如何建模模型训练花费时间，设计了一个自顶向下的分解策略。</p>
<h3 id="顶数据并行">顶：数据并行</h3>
<p>首先是数据并行的维度，即先考虑把哪些显卡视为一组，组之间流水线并行，组之内张量并行+流水线并行。这是分解中的顶层。此时训练（一轮）的时间可以用下列等式表示：</p>
<p><span class="math display">\[\begin{equation}
\label{eq1}
训练时间=\max(各组训练时间) + \max(各组反向传播时间)
\end{equation}\]</span></p>
<h3 id="中流水线并行">中：流水线并行</h3>
<p>之后是流水线并行的维度，即考虑怎样把模型中一层一层的神经网络组合成一个阶段，而一个阶段会在最后张量并行的分解后，由一张或多张显卡负责。即对于某一组组内训练（一轮）的时间可表示为：</p>
<p><span class="math display">\[\begin{equation}
\label{eq2}
\begin{aligned}
某组训练时间=&amp;(微批次大小-1) \cdot \max(各阶段训练时间) \\
           &amp; + \sum 各阶段计算时间 \\
           &amp; + \sum 各阶段间通讯时间
\end{aligned}
\end{equation}\]</span></p>
<p>式<span class="math inline">\(\eqref{eq2}\)</span>中，<span
class="math inline">\((微批次大小-1) \cdot
max(各阶段训练时间)\)</span>是为了刻画“短板效应”（原文straggler
effect，直译掉队者效应），即当微批次大小大于1时，总时长会以流水线中花费时长最长的那个时间的相应倍数增加。</p>
<h3 id="底张量并行">底：张量并行</h3>
<p>最后，在最底层的是张量并行的维度，不过以论文的方法进行到这一步时它的维度其实就是简单的（总显卡数÷数据并行维度÷流水线并行维度）。分解到这里，我们就可以实际考察的是对于某一层，它的计算时间和与下一层的通讯时间如何建模了</p>
<h3 id="代价建模采样">代价建模：采样</h3>
<p>对于计算时间，论文指出如果用（计算量÷显卡浮点计算能力）的方式代替，在计算量较大的时候这种拟合还算贴合实际，但当张量并行的维度逐渐变高之后，导致的误差就会干扰预测的结果。所以论文提出的替代方法是实际对多个张量并行设置进行性能分析，并将结果存储在查找表中。这种方法理论上对于一个特定模型和特定型号显卡，计算时间是相同的，因此论文认为这种方法是可行的。</p>
<p>对于通讯时间，论文则是使用（集合通讯的数据量÷带宽）计算，没有其它的特别处理。（但伏笔+1）</p>
<p>获得了两个时间的具体表达方法之后，就可以倒着从低向上，通过式<span
class="math inline">\(\eqref{eq1}\)</span>和式<span
class="math inline">\(\eqref{eq2}\)</span>估算模型训练（一轮的）时间了。</p>
<h2
id="流水线并行的层划分动态规划算法">流水线并行的层划分：动态规划算法</h2>
<p>这部分则是论文（我认为）最主要的创新点。论文提出了一个问题：对于某模型，给定三种并行的策略和微批次大小，如果可以进行划分的话，怎样将所有层划分成多个阶段，并保证某一个组内的训练时间（式<span
class="math inline">\(\eqref{eq2}\)</span>）最小？（需要特意指出的是，此时这一个组内具体可用显卡和它们的连接方式，即计算能力和通讯带宽是已经确定的）</p>
<p>这部分正是设计的动态规划算法试图解决的部分，同时也让论文的方法区别于了彻底的穷举法。</p>
<p>为了方便打字，这一部分将会使用大量的符号，而不像之前那样用意义代替符号表达式。为方便阅读，这里先给出所有涉及的符号和它的意义：</p>
<table>
<thead>
<tr>
<th style="text-align: center;">符号</th>
<th style="text-align: center;">意义</th>
<th style="text-align: center;">论文符号</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;"><span
class="math inline">\(mbs\)</span></td>
<td style="text-align: center;">微批次大小</td>
<td style="text-align: center;"><span
class="math inline">\(gas\)</span></td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(L\)</span></td>
<td style="text-align: center;">神经网络总层数</td>
<td style="text-align: center;"><span
class="math inline">\(L\)</span></td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(S\)</span></td>
<td style="text-align: center;">（策略划分的）阶段数</td>
<td style="text-align: center;"><span
class="math inline">\(k\)</span>、<span
class="math inline">\(pp\)</span></td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(cost^{layer}_{e,i}\)</span></td>
<td style="text-align: center;">第i层的计算时间</td>
<td style="text-align: center;"><span
class="math inline">\(t^{layer}_i\)</span></td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(cost_{e,i}\)</span></td>
<td style="text-align: center;">第i阶段的计算时间</td>
<td style="text-align: center;"><span
class="math inline">\(t_i\)</span></td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(cost^{layer}_{e,i}\)</span></td>
<td style="text-align: center;">第i和i+1层之间的通讯时间</td>
<td style="text-align: center;">无</td>
</tr>
<tr>
<td style="text-align: center;"><span
class="math inline">\(cost_{c,i}\)</span></td>
<td style="text-align: center;">第i和i+1阶段之间的通讯时间</td>
<td style="text-align: center;"><span
class="math inline">\(e_i\)</span></td>
</tr>
</tbody>
</table>
<p>不过对于<span
class="math inline">\(cost^{layer}_{e,j}\)</span>和<span
class="math inline">\(cost^{layer}_{c,j}\)</span>是怎么得来的，这里也有一个小小的伏笔。接下来开始正式介绍该算法。</p>
<p>设一个神经网络模型有<span
class="math inline">\(L\)</span>层，且策略指定分成指定的<span
class="math inline">\(S\)</span>个计算阶段，引入一个辅助变量<span
class="math inline">\(m\)</span>和它的取值空间<span
class="math inline">\(M\)</span>，则对于子问题<span
class="math inline">\(i\)</span>层和<span
class="math inline">\(j\)</span>个阶段的训练时间，可以把式<span
class="math inline">\(\eqref{eq2}\)</span>写成动态规划数组：</p>
<p><span class="math display">\[\begin{equation}
dp[i][j][m] = (mbs-1)\cdot \max(0,\max_{1\leq j&#39; \leq
j}(cost_{e,j&#39;})-m)+\sum_{j&#39;=1}^{j-1}cost_{c,j&#39;}+\sum_{j&#39;=1}^{j}cost_{e,j&#39;}
\end{equation}\]</span></p>
<p>注意引入变量<span
class="math inline">\(m\)</span>后与原式的差异，它的作用会在后续解释。但可以看出当<span
class="math inline">\(i=L,j=S,m=0\)</span>时，该数组会给出上述问题的求解。</p>
<p>进一步给出递推表达式如下：</p>
<p><span class="math display">\[\begin{equation}
\begin{aligned}
dp[i][j][m] =&amp; \min_{ j \leq i&#39; \leq i-1,m \in
M}(dp[i&#39;][j-1][max(\sum_{i&#39;&#39;=i&#39;+1}^{i}cost^{layer}_{e,i&#39;&#39;},m)]
\\
    &amp;+(mbs-1)\cdot
max(0,\sum_{j&#39;=i&#39;+1}^{i}cost^{layer}_{e,j&#39;} -m) \\
    &amp;+\sum_{i&#39;&#39;=i&#39;+1}^{i}cost^{layer}_{e,i&#39;&#39;}
+cost^{layer}_{c,i&#39;})
\end{aligned}
\end{equation}\]</span></p>
<p>本文将由浅入深讲解该递推表达式是如何得出的。但解释变量<span
class="math inline">\(m\)</span>需要一点篇幅，所以先把变量<span
class="math inline">\(m\)</span>放一边，即动态规划数组退化成<span
class="math inline">\(dp[i][j]\)</span>的情况。</p>
<h3 id="当不存在变量m时会有什么问题">当不存在变量<span
class="math inline">\(m\)</span>时会有什么问题</h3>
<p>只要提前告诉你这个问题能动态规划算法解决，想出来如下一个递推式还是比较容易的。对于<span
class="math inline">\(dp[i][j]\)</span>，它的值可以如下拆分：</p>
<figure class="highlight text"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">  &quot;前半&quot;   cost^layer_&#123;c,i&#x27;&#125;  &quot;后半&quot;</span><br><span class="line">1,2......i&#x27;&lt;---------------&gt;i&#x27;+1......i</span><br><span class="line">\_________/        ^        \_________/</span><br><span class="line">  i&#x27;个层          切断       i&#x27;+1到i层</span><br><span class="line"> j-1个阶段                    第j阶段</span><br></pre></td></tr></table></figure>
<p>当切断在<span class="math inline">\(i&#39;\)</span>时：</p>
<ul>
<li>对于前半部分，自然就是<span
class="math inline">\(dp[i&#39;][j-1]\)</span></li>
<li>对于后部分，也可以自然想到是<span
class="math inline">\(\sum_{i&#39;&#39;=i&#39;+1}^{i}t^{layer}_{e,i&#39;&#39;}\)</span></li>
<li>再者就是需要加上<span class="math inline">\(i&#39;\)</span>和<span
class="math inline">\(i&#39;+1\)</span>之间的通信时间</li>
<li>嗯？短板效应怎么表达？前半计算时间最大的阶段似乎没有提供？我怎么才能知道它在前半还就是后半？</li>
</ul>
<p>短板效应如何计算的问题，个人有个想法是每轮更新的时候实际记录当前计算时间的最高的阶段的值，相当于顺便做一个类似子序列最大值的动态规划（甚至应该可以把后文的时间复杂度压到<span
class="math inline">\(O(S\cdot L^2)\)</span>）。</p>
<p>但回到论文本身，这就是引入辅助变量<span
class="math inline">\(m\)</span>试图解决的问题了。</p>
<h3 id="如何正确地计算短板效应">如何正确地计算短板效应</h3>
<p>首先介绍变量<span
class="math inline">\(m\)</span>是什么，它的取值空间是什么：它实际的意义是所有可能的阶段、即任意单个或一段连续层的总计算时间，外加上取值<code>0</code>（虽然实际上会遇到有的层的计算时间就是0，但点明取值有0是保证算法的正确性）。</p>
<p>设本次和上次的切分的位置分别为<span
class="math inline">\(i&#39;\)</span>和<span
class="math inline">\(i&#39;&#39;\)</span>，同样的本次和上次的后半层的总计算时间为<span
class="math inline">\(t_{i&#39;&#39;}\)</span>和<span
class="math inline">\(t_{i&#39;}\)</span>。此时稍将递推式中前半的数组展开一次就会得到一个类似于如下的表达式：</p>
<p><span class="math display">\[\begin{equation}
...+(mbs-1)\cdot(max(0,t_{i&#39;&#39;}-max(0,t_{i&#39;}))+max(0,t_{i&#39;}-m))...
\end{equation}\]</span></p>
<p>再加上一个引理：</p>
<p><span class="math display">\[\begin{equation}
\label{eq:lemma}
\begin{aligned}
&amp;max\{0, t_{i&#39;&#39;} - max\{t_{i&#39;}, m\}\} + max\{0,
t_{i&#39;} - m\}\\
=&amp;max\{0, max\{t_{i&#39;&#39;}, t_{i&#39;}\} - m\}
\end{aligned}
\end{equation}\]</span></p>
<p>脑补一下<span
class="math inline">\(dp[L][S][m]\)</span>完全展开的样子，再将式<span
class="math inline">\(\eqref{eq:lemma}\)</span>带入，会大致得到如下一个东西：</p>
<p><span class="math display">\[\begin{equation}
...+(mbs-1)\cdot max(0,\max_{1\leq j\leq S}(cost^{layer}_{e,j}-m))...
\end{equation}\]</span></p>
<p>再带入<span
class="math inline">\(m=0\)</span>，很显然这就是所需计算的短板效应。</p>
<p>对于引理的证明，简单来讲就是一个分类讨论，可以去看原论文的附录<del>绝对不是连复制粘贴都懒得干了</del>。此时还待解决的问题就只有初始值了，但初始值的计算非常简单，也直接省略了。</p>
<h3 id="小细节">小细节</h3>
<p>首先最明显容易注意的是对于所有可能的计算时间<span
class="math inline">\(m\)</span>，因为它肯定不是一个像是<code>0,1,2,3...</code>的连续序列，设计动态规划数组的时候显然不会真的直接用<span
class="math inline">\(m\)</span>作为数组的下标。</p>
<p>在实际写代码的时候，会用一个对象<code>possible</code>包装变量<code>m</code>，即<code>possible[m]</code>和<code>m=possible.index(cur_sum)</code>的方式来表达<code>m</code>。考虑到有已知取值，反过来求下标的需求，这个对象可以是去重的有序数组、查找树甚至哈希表。论文源代码选取的是python中的<code>set</code>去重后转化为<code>list</code>对象。</p>
<p>另外一个容易注意到的问题是<span
class="math inline">\(dp[L][S][0]\)</span>似乎只能得到最小的总时间，但在实际训练时自然是需要具体的阶段切分方式的。</p>
<p>联想给出具体的最短路径的Dijkstra算法，可以想到实际编程的时候，动态规划数组不需要一定是一个数字型。</p>
<p>一种方式是可以是一个包含了第<span
class="math inline">\(j\)</span>阶段具体切在何处的结构体<code>{i',time}</code>，再通过<code>i'</code>处的最佳<code>j-1</code>切分依次回向查找。但这里会遇到一个问题是<code>i'</code>处最佳<code>j-1</code>切分的<code>m</code>值如何确定，这涉及到上次的<code>m</code>和本次切分“后半”的计算总时间,递推计算表达式太复杂<del>太懒</del>就不具体给出了。</p>
<p>另一种是论文源代码实现的方式<code>{partion,time}</code>，即直接在上一次的划分信息上加上本次的划分信息<code>{i',i'+1...}</code>。这样做的好处是可以做成滚动数组（虽然粗算了一下可能节省不了存储空间，论文源代码也确实没做），当然最重要的是最后提取切分方式也足够简单直观。</p>
<h3 id="复杂度分析">复杂度分析</h3>
<p><strong>空间复杂度：</strong>
由于求解的过程发生在实际训练前，而对于能训练当代神经网络的设备，内存是完全管够的。加上根据代码实现<del>应该</del>略有不同，这里就省略了。</p>
<p><strong>时间复杂度：</strong> 首先对于辅助变量<span
class="math inline">\(m\)</span>，显然有<span
class="math inline">\(\binom{L-1}{2}\)</span>可能值，即时间复杂度为<span
class="math inline">\(O(L^2)\)</span>。 动态规划每一次更新需要考虑<span
class="math inline">\(O(L)\)</span>个切分位置。再算上动态规划数组<span
class="math inline">\(dp[i][j][m]\)</span>还有另外两个维度，分别是<span
class="math inline">\(O(L)\)</span>和<span
class="math inline">\(O(S)\)</span>。最后可以得到总的时间复杂度<span
class="math inline">\(O(S\cdot L^4)\)</span>，这显然比起穷举所有<span
class="math inline">\(S\)</span>个切分位置的<span
class="math inline">\(\binom{L-1}{S-1}\)</span>好上太多了。</p>
<h1 id="实验效果">实验&amp;效果</h1>
<p>这部分不是关注重点，就简单提一下了。对具体数字感数字的话可以点开论文原文看看。</p>
<p>首先论文用的是发表那会流行的Megatron-Deepspeed框架来实现3D并行。当然阅读源码的话，为了支持当时还不支持的自定义流水线阶段划分对框架进行了一些修改，这一部分也一并提供在源代码的仓库中了。</p>
<p>下表是<del>纯偷懒</del>做的表格，可以对具体效果有一个大致的了解。</p>
<table>
<colgroup>
<col style="width: 2%" />
<col style="width: 32%" />
<col style="width: 20%" />
<col style="width: 16%" />
<col style="width: 27%" />
</colgroup>
<thead>
<tr>
<th style="text-align: center;">实验名称</th>
<th style="text-align: center;">集群配置</th>
<th style="text-align: center;">模型配置</th>
<th style="text-align: center;">对比对象</th>
<th style="text-align: center;">效果</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">同构环境</td>
<td style="text-align: center;">4×AWS EC2 g4dn.12xlarge节点，每节点4×T4
GPU，节点内50Gbps PCIe，节点间50Gbps</td>
<td style="text-align: center;">GPT-2 (L=24, H=1024)，batch size=32</td>
<td style="text-align: center;">Megatron, AMP, SA(模拟退火)</td>
<td
style="text-align: center;">Megatron最优1.32s；AMP最优1.20s；SA最优1.57s。AMP能找到与调优的策略相当或略优的结果</td>
</tr>
<tr>
<td style="text-align: center;">集群异构</td>
<td style="text-align: center;">3×AWS EC2 p3.8xlarge (4×V100, 170Gbps
NVLink节点内, 10Gbps节点间) + 1×g4dn.12xlarge (4×T4)</td>
<td style="text-align: center;">GPT-2 (L=24, H=1024)，batch size=32</td>
<td style="text-align: center;">Megatron-LM, RS(随机搜索), AMP, SA</td>
<td
style="text-align: center;">Megatron-LM最优1.97s；RS最优1.71s；AMP最优1.28s；SA最优1.46s。AMP相比Megatron提升1.54×</td>
</tr>
<tr>
<td style="text-align: center;">模型异构</td>
<td style="text-align: center;">4×AWS EC2 p3.8xlarge (4×V100)</td>
<td style="text-align: center;">改进版TransGAN：12层dim=1024 +
12层dim=16，batch size=64</td>
<td style="text-align: center;">Megatron-LM, AMP w/参数均衡, AMP
w/层数均衡, AMP</td>
<td
style="text-align: center;">Megatron-LM最优1.89s；参数均衡2.19s；层数均衡1.25s；AMP最优1.07s。AMP相比Megatron提升1.77×</td>
</tr>
</tbody>
</table>
<h1 id="不足之初思考">不足之初&amp;思考</h1>
<p>首先介绍<del>翻译</del>论文原文中已经点明的待改进的问题：</p>
<ol type="1">
<li>没有考虑显卡能不能装下分割后的模型训练。
<ul>
<li>不过这对于使用非TensorFlow的神经训练框架是常见的，而且也是一个常见的改进方向。</li>
<li>但是由于分布式方法尚未彻底成熟的现况，也只能是针对个别大模型训练框架，不仅是劳神费力做出来，还需要一个长期痛苦的维护。不如说这件事只能期待提供框架背后的团队。</li>
</ul></li>
<li>流水线阶段划分的算法时间复杂度<span class="math inline">\(O(S\cdot
L^4)\)</span>对于目前模型层数<span
class="math inline">\(L\)</span>越来越夸张的趋势来说不太妙。
<ul>
<li>论文提到可以考虑一些例如operator clustering的策略来让分解之后的<span
class="math inline">\(L\)</span>尽量低。</li>
<li>当然原文中说流水线划分在整个优化过程中占比太大的问题，<del>个人觉得有一部分是得怪在python身上</del>。</li>
</ul></li>
</ol>
<p>然后是一些个人提出的问题：</p>
<ol start="3" type="1">
<li>把显卡的通信带宽认为是定值是不太现实的。
<ul>
<li>正如模型有越切越小，实际计算时间会越偏离单纯的计算复杂度÷计算能力，通信的时候也会出现这个问题。</li>
</ul></li>
<li>论文的优化策略，主体仍然是引用的启发式算法。
<ul>
<li>也就是虽然论文提出的流水线划分算法确实精妙，但其解决的问题太过具体，也很难再扩展到同时考虑3D的优化上。</li>
</ul></li>
<li>（虽然是琢磨源码的时候看出来的）默认使用按顺序生成的显卡编号<code>Global Rank</code>，在流水线阶段划分的时候默认使用了这个顺序。
<ul>
<li>尽管启发式算法会倾向在集群的节点内做张量+流水线并行，但如果涉及到同时节点内间不同带宽的显卡通信的情况，是一定会影响本文的流水线阶段划分算法最优的正确性的。</li>
<li>而且就算是节点内显卡通信带宽不一致的情况也是很有可能的。比如服务器上常见的多CPU配置，两个CPU下属的显卡间通信就会在内存中多中转一次。（<del>加钱用NVLink可破</del>）</li>
</ul></li>
<li>（也是看代码的时候看出来的）代价模型中的计算时间本质上还是一轮训练的采样。
<ul>
<li>正因为还是采样，如果集群的配置出现了再小的会干扰显卡运行<del>干活</del>的变动，也会导致代价模型的预测可信度变低。一时想不到如何改进的话，也可以尝试说明策略对采样的敏感度如何。</li>
</ul></li>
</ol>
<h1 id="扩展阅读">扩展阅读</h1>
<ul>
<li><a href="">DLC：论文的源码分析</a>
<ul>
<li><del>会写的</del>，但可以先看看这个<a
target="_blank" rel="noopener" href="https://github.com/Ariasuko/AMP">复现的时候顺手重构的</a></li>
</ul></li>
<li><a href="">如何确定论文中的通信带宽？</a>
<ul>
<li><del>一定会写的</del></li>
</ul></li>
</ul>

      
    </div>

    
    
    

    <footer class="post-footer">
        <div class="post-eof"></div>
      
    </footer>
  </article>
</div>





</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2026</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">Suko Aria</span>
  </div>
  <div class="powered-by">由 <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>

</body>
</html>
